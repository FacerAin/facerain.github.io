---
isTIL: true
date: "2022-10-05"
title: "10월 5일 회고"
categories: ["Review"]
summary: "10월 5일을 회고합니다."
thumbnail: "./22-10-05-th.jpg"
---


## 오늘 한 일
- Generative Model
  - Independent, Condition Independent
  - VAE, GAN, Diffusion
  - 사실 잘 모르겠다..
- Dacon 코드 유사도 대회 진행
  - Colab 사양이 애매하다
  - huggingface가 매우 강력하다. 전처리의 중요성을 다시금 느낌
  - 데이터에 맞는 토크나이저를 사용하는 것만으로도 성능이 10% 향상된다
  - 전처리 코드에 집중해보자
  - 상위권 코드들을 보니 **contrastive learning**을 많이 사용한다
    - 이 부분에 대한 추가 공부 필요
  - 기록하면서 학습하는 습관 Good
- 데이터 시각화도 공부
  - 잉크량 비례의 법칙이 와닿았다
    - 화려한 시각화보다 적절한 시각화가 중요
    - 인과관계 VS 상관관계
- Leetcode Sliding window maximum
  - Monotone Queue에 대해 공부할 수 있었다.
  - 몇번씩 볼 유형이니 잘 알아두자
- 오늘의 팀 질문
  - **Q1. 모델을 inference할 때 model.eval()과 with torch.no_grad()의 차이점**
  - A1. model.eval은 기울기 계산 비활성화 뿐만 아니라, dropout과 batchnorm 연산도 비활성화 시킨다
    -  batchnorm 연산은 inference 단계에서 결과에 영향을 미치지 않는다 [참고](https://towardsdatascience.com/speed-up-inference-with-batch-normalization-folding-8a45a83a89d8)
   -  **Q2. 1x1 합성곱 layer는 왜 사용?**
   -  A2. 채널 수 조절, 계산량 감소, 비선형성
   -  **Q3. Small batch training의 장점**
   -  A3. Generalization performance와 Training Stability가 좋아짐 [참고](https://blog.lunit.io/2018/08/03/batch-size-in-deep-learning/)
   -  **Q4. L2 regularization과 batch norm을 같이 사용하면?**
   -  A4. 효과가 없다. [참고](https://blog.janestreet.com/l2-regularization-and-batch-norm/)

## 오늘의 한 문장
단지 시작하는 것이다.  
한 번에 한 사람씩 (마더 테레사)