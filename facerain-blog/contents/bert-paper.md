---
isTIL: false
date: "2022-02-27"
title: "[ë…¼ë¬¸ ë¦¬ë·°] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
categories: ["NLP", "Paper"]
summary: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ë…¼ë¬¸ì„ ë¦¬ë·°í•©ë‹ˆë‹¤."
thumbnail: "./bert-paper/th.jpg"
---

ì´ë²ˆ ì‹œê°„ì—ëŠ” [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) ë…¼ë¬¸ì„ ë¦¬ë·°í•©ë‹ˆë‹¤.  
BERTëŠ” ë“±ì¥ê³¼ ë™ì‹œì— Question Answering ë“± ë‹¤ì–‘í•œ NLP ë¬¸ì œë“¤ì— ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©° NLP ë¶„ì•¼ì— í•œ íšì„ ê·¸ì—ˆìŠµë‹ˆë‹¤.

![bert-cite](./bert-paper/bert-usage.png "bert ëª¨ë¸ì˜ usage ì¶”ì´ ì¶œì²˜: paperswithcode")
ìœ„ ê·¸ë¦¼ì€ bert ëª¨ë¸ì˜ ì‚¬ìš© ì¶”ì´ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ì…ë‹ˆë‹¤. 2022ë…„ í˜„ì¬ê¹Œì§€ë„ ë§ì€ ë…¼ë¬¸ì—ì„œ BERT ëª¨ë¸ì„ ë‹¤ë£¨ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì²˜ëŸ¼ BERT ëª¨ë¸ì€ ìµœì‹  NLP íŠ¸ë Œë“œë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œ í•„ìˆ˜ë¼í•´ë„ ê³¼ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤. ê·¸ëŸ¼ BERTì— ëŒ€í•´ í•¨ê»˜ ì•Œì•„ë³´ë„ë¡ í•©ì‹œë‹¤!

![bert](./bert-paper/bert.jpg "BERTì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤!")

## Abstract

- ì—°êµ¬íŒ€ì€ BERT(**Bidirectional Encoder Representations from Transformers**) ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤.
- BERTëŠ” Deep bidirectional representation ëª¨ë¸ë¡œ unlabeled textë¥¼ **ì–‘ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•œ ê²ƒì´ íŠ¹ì§•** ì…ë‹ˆë‹¤. (ELMOì™€ GPT-1ê³¼ì˜ ì£¼ìš”í•œ ì°¨ì´ì )
- ëª¨ë¸ì€ downstream taskë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í° ëª¨ë¸ ìˆ˜ì • ì—†ì´ **ë‹¨ì§€ í•˜ë‚˜ì˜ ì¶œë ¥ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ íŒŒì¸íŠœë‹ì„ ì§„í–‰** í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- íŠ¹íˆ íŒŒì¸íŠœë‹í•œ bert ëª¨ë¸ì€ **11ê°œ NLP Taskì—ì„œ SOTA ì„±ëŠ¥ì„ ë‹¬ì„±** í•  ì •ë„ë¡œ ì„±ëŠ¥ì´ ì¢‹ì€ ê²ƒì´ íŠ¹ì§•ì…ë‹ˆë‹¤.

## Introduction

ìµœê·¼ ë§ì€ NLP Taskì—ì„œ pre-trained ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. pre-trained ëª¨ë¸ì´ ì„±ëŠ¥ì´ ì¢‹ê³  íš¨ìœ¨ì ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.  
![bert](./bert-paper/0.png "BERT ëª¨ë¸ êµ¬ì¡°")
pre-trained ì–¸ì–´ ëª¨ë¸ì„ downstream-taskì— ì ìš©í•˜ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” í¬ê²Œ ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.
**ë°”ë¡œ Feature basedì™€ Fine Tuningì…ë‹ˆë‹¤.** ë‘ ë°©ë²•ì˜ íŠ¹ì§•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

### Feature based

- **ì–´ë–¤ íŠ¹ì • taskë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ architecturesë¥¼ ë§Œë“¤ê³ , ì—¬ê¸°ì— pre-trained representationsë¥¼ ì¶”ê°€ì ì¸ featureë¡œ ì œê³µí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.**
- ëŒ€í‘œì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” ELMOê°€ ìˆìŠµë‹ˆë‹¤.

### Fine Tuning

- **íŠ¹ì • taskë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„°ëŠ” ìµœì†Œí•œìœ¼ë¡œ ë‘ê³ , pre-trained parameterë¥¼ downstream taskë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.**
- ëŒ€í‘œì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” gpt-1ì´ ìˆìŠµë‹ˆë‹¤.

![direction](./bert-paper/1.png "BERT VS GPT VS ELMO")

ìœ„ì—ì„œ ì–¸ê¸‰í•œ ELMOì™€ GPT-1 ëª¨ë‘ ë‹¨ë°©í–¥ êµ¬ì¡°ë¼ëŠ” ê³µí†µì ì´ ìˆìŠµë‹ˆë‹¤.

> Q. ELMO êµ¬ì¡°ë¥¼ ë³´ë©´ ì–‘ë°©í–¥ ì•„ë‹Œê°€ìš”?  
> A. bi-LSTM êµ¬ì¡°ë¥¼ ìì„¸íˆ ë³´ë©´ left-to-right LSTMê³¼ right-to-left LSTMì´ ê²°í•©ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ ë‘ê°œì˜ LSTMì€ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµë˜ì–´ ê²°í•©ë˜ëŠ”ë°, ì´ëŠ” ê·¼ë³¸ì ìœ¼ë¡œëŠ” ì–‘ë°©í–¥ì´ ì•„ë‹Œ ë‹¨ë°©í–¥ ëª¨ë¸ì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì—°êµ¬íŒ€ì€ **ë‹¨ë°©í–¥ êµ¬ì¡°ê°€ pre-trained ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì œí•œ** í•œë‹¤ê³  ìƒê°í•˜ì˜€ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì—°êµ¬íŒ€ì€ **ëª¨ë“  ë ˆì´ì–´ì—ì„œ ì–‘ë°©í–¥ í•™ìŠµì„ í•  ìˆ˜ ìˆëŠ” bertë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.**  
ì´ë¥¼ ìœ„í•´ bertì—ëŠ” í¬ê²Œ **ë‘ê°€ì§€ í•™ìŠµ Taskë¥¼ ì§„í–‰** í•©ë‹ˆë‹¤.

### MLM(Masked Laguage Model)

ê¸°ì¡´ Language Modelì˜ ê²½ìš° ì£¼ì–´ì§„ ë‹¨ì–´ ì‹œí€€ìŠ¤ë¥¼ í†µí•´ ê·¸ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” Taskë¡œ í•™ìŠµí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ë•Œ í˜„ì¬ ì…ë ¥ ë‹¨ì–´ ì´í›„ì˜ ë‹¨ì–´ë“¤ì˜ ì •ë³´ë¥¼ ëª¨ë¸ì—ê²Œ ì•Œë ¤ì¤„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.(left-to-right, ë‹¨ë°©í–¥)

ê·¸ì— ë°˜í•´ MLM(Masked Laguage Model)ì€ ë¬¸ì¥ì—ì„œ ë¬´ì‘ìœ„ í† í°ì— ë¹ˆì¹¸(MASK)ì„ í•´ë†“ìœ¼ë©´ ë¹ˆì¹¸(Mask)ì— ë§ëŠ” ë‹¨ì–´ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. Transformer encoderì— íŠ¹ì • í† í°ì— MASK ì²˜ë¦¬ë¥¼ í•œ ë¬¸ì¥ì„ í•œë²ˆì— ë„£ê³  MASK ë‹¨ì–´ì˜ ì •ë‹µì„ ì°¾ê²Œ í•˜ë„ë¡ í•©ë‹ˆë‹¤. **ì´ëŠ” Mask ë‹¨ì–´ì˜ left, right contextë¥¼ ëª¨ë‘ í™œìš©í•´ì•¼ í•˜ë¯€ë¡œ deep-bidirectional í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.**

### NSP(Next Sentence Prediction)

**ë‘ ê°œì˜ ë¬¸ì¥ì´ ì´ì–´ì§€ëŠ” ë¬¸ì¥ì¸ì§€ ëª¨ë¸ì´ ë§ì¶”ë„ë¡ í•˜ëŠ” Taskì…ë‹ˆë‹¤.** ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ ì„¹ì…˜ì—ì„œ ë‹¤ì‹œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

## Model Architecture

BERTëŠ” Transformerì—ì„œ **Encoder ë¶€ë¶„ë§Œì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì„±**ë©ë‹ˆë‹¤.
Transformerê°€ ê¶ê¸ˆí•˜ì‹  ë¶„ì€ [Transformer ë…¼ë¬¸ ë¦¬ë·°](https://facerain.club/transformer-paper/)ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”!  
ì—°êµ¬íŒ€ì€ BERT_baseì™€ BERT_largeë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ì„ êµ¬ì¶•í•˜ì˜€ìŠµë‹ˆë‹¤.
ê° ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. Lì€ ë ˆì´ì–´ì˜ ê°œìˆ˜, HëŠ” íˆë“  ë ˆì´ì–´ì˜ í¬ê¸°, AëŠ” self-attentionì˜ headì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

- BERT_base: L=12, H=768, A=12, Total Parameter=110M
- BERT_large L=24, H=1024, A=16, Total Parameter=340M

ì´ë•Œ **BERT-baseëŠ” OpenAI GPTì™€ì˜ ë¹„êµë¥¼ ìœ„í•´ ê°™ì€ ëª¨ë¸ ì‚¬ì´ì¦ˆë¥¼ ì±„íƒ**í•˜ì˜€ìŠµë‹ˆë‹¤.

## Input Representation

BERTì˜ ì…ë ¥ì€ ì„¸ê°€ì§€ ì„ë² ë”©ì´ ë”í•´ì ¸ êµ¬ì„±ë©ë‹ˆë‹¤.
![input](./bert-paper/2.png "BERTì˜ Input")

- **Token Embedding**
  - í† í°ì— WordPiece Embeddingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
  - [CLS]ëŠ” ë¬¸ì¥ì˜ ì‹œì‘ì„ ì˜ë¯¸í•˜ëŠ” ìŠ¤í˜ì…œ í† í°ìœ¼ë¡œ, ë¶„ë¥˜ Taskì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
  - ë¬¸ì¥ì˜ êµ¬ë¶„ì„ ì˜ë¯¸í•˜ëŠ” [SEP], MLMì—ì„œ Maskí•  í† í°ì„ ì˜ë¯¸í•˜ëŠ” [MASK], ë°°ì¹˜ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ê¸° ìœ„í•œ [PAD] ë“±ì˜ í† í°ì´ ìˆìŠµë‹ˆë‹¤.
- **Segment Embedding**

  - ì…ë ¥ ë¬¸ì¥ ìŒì—ì„œ ì• ë¬¸ì¥ì—ëŠ” A embedding, ë’¤ì˜ ë¬¸ì¥ì—ëŠ” b embeddingì„ ë”í•´ì¤ë‹ˆë‹¤.
  - ì´ë•Œ ë”í•´ì£¼ëŠ” ì„ë² ë”© ê°’ì€ ê³ ì •ëœ ê°’ì…ë‹ˆë‹¤.
  - ë§Œì•½ ì…ë ¥ ë¬¸ì¥ì´ ë‹¨ì¼ ë¬¸ì¥ì¼ ê²½ìš°ì—ëŠ” A embeddingë§Œ ë”í•´ì£¼ë©´ ë©ë‹ˆë‹¤.

- **Positional Embedding**
  - Transformerì™€ ë§ˆì°¬ê°€ì§€ë¡œ Tokenì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì£¼ì…í•˜ê¸° ìœ„í•´ Positional Embeddingì„ ë”í•´ì¤ë‹ˆë‹¤.

## Pre-Training

BERTëŠ” 2ê°€ì§€ pre-train íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ë°”ë¡œ ì•ì„œ ì‚´í´ë³¸ MLM(Masked LM)ê³¼ NSP(Next Sentence Prediction)ì…ë‹ˆë‹¤. ê°ê°ì˜ Taskë¥¼ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### 1. MLM(Masked Language Model)

ë³¸ ì—°êµ¬ì—ì„œ GPTë‚˜ ELMOê°€ ê°€ì§€ëŠ” ë‹¨ë°©í–¥ í•™ìŠµ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ MLMì„ ë„ì…í•˜ì˜€ìŠµë‹ˆë‹¤. ëœë¤ìœ¼ë¡œ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ì—¬ ëª¨ë¸ì´ í•´ë‹¹ ë§ˆìŠ¤í‚¹ ëœ í† í°ì˜ ì›ë˜ ë‹¨ì–´ë¥¼ ë§ì¶”ë„ë¡ í•˜ëŠ” Taskì…ë‹ˆë‹¤.

MLMì— í•„ìš”í•œ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ì ˆì°¨ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- ì „ì²´ í•™ìŠµ ë°ì´í„° í† í°ì˜ 15%ì„ ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í† í°ìœ¼ë¡œ ì„ ì •í•©ë‹ˆë‹¤.
- ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í† í° ê°€ìš´ë° 80%ëŠ” [Mask] í† í°ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤. (my dog is hairy -> my dog is [MASK])
- ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í† í° ê°€ìš´ë° 10%ëŠ” í† í°ì„ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤. (my dog is hairy -> my dog is hairy)
- ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í† í° ê°€ìš´ë° 10%ëŠ” í† í°ì„ ëœë¤ìœ¼ë¡œ ë‹¤ë¥¸ í† í°ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. (my dog is hairy -> my dog is apple)

MLMì€ ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” pre-training taskë¥¼ ìˆ˜í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.

> Q. ì™œ ëª¨ë“  ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ì§€ ì•Šê³  ì¼ë¶€ëŠ” ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ë°”ê¾¸ë‚˜ìš”?  
> A. [Mask] í† í°ì€ pre-train ê³¼ì •ì—ì„œë§Œ ì‚¬ìš©í•˜ê³ , fine-tuning ê³¼ì •ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” ë‘ ê³¼ì • ì‚¬ì´ì˜ ë¶ˆì¼ì¹˜(mismatch)ë¥¼ ìœ ë°œí•˜ì—¬ downstream taskë¥¼ ìˆ˜í–‰í•  ë•Œ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë§ˆìŠ¤í‚¹ ëŒ€ìƒ í† í°ì„ ëœë¤ìœ¼ë¡œ ë‘ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘¬ì„œ ëª¨ë¸ì´ ëª¨ë“  ë‹¨ì–´ ì‚¬ì´ì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ê´€ê³„ë¥¼ ì„¸ë°€íˆ ì‚´í•„ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

### 2. NLP (Next Sentence Prediction)

Question Answering(QA)ë‚˜ Natural Language Infrence(NLI)ì™€ ê°™ì€ Taskì—ì„œëŠ” **ë‘ ë¬¸ì¥ ì‚¬ì´ì˜ ê´€ê³„(Relationship)ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.** í•˜ì§€ë§Œ ê¸°ì¡´ Languauge Modelì—ì„œëŠ” ë¬¸ì¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê³ ì BERTëŠ” **NSPë¥¼ í†µí•´ ë¬¸ì¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.**

NSPì— í•„ìš”í•œ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ì ˆì°¨ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- ëª¨ë“  í•™ìŠµ ë°ì´í„°ëŠ” 1ê±´ë‹¹ ë¬¸ì¥ ë‘ê°œë¡œ êµ¬ì„±ëœ ë¬¸ì¥ ìŒìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
- í•™ìŠµ ë¬¸ì¥ìŒ ë°ì´í„° ì¤‘ 50%ëŠ” ë™ì¼í•œ ë¬¸ì„œì—ì„œ ì‹¤ì œ ì´ì–´ì§€ëŠ” ë¬¸ì¥ìŒì„ ì„ ì •í•©ë‹ˆë‹¤. ì •ë‹µ Labelë¡œëŠ” ì°¸(IsNext)ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
- ë‚˜ë¨¸ì§€ 50%ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œì—ì„œ ëœë¤ìœ¼ë¡œ ë½‘ì•„ ë¬¸ì¥ìŒì„ êµ¬ì„±í•©ë‹ˆë‹¤. (ê´€ê³„ê°€ ì—†ëŠ” ë¬¸ì¥ ë§Œë“¤ê¸°) ì •ë‹µ Labelë¡œëŠ” ê±°ì§“(NotNext)ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

ì´ì œ ëª¨ë¸ì—ì„œëŠ” ë¬¸ì¥ ìŒì´ ì°¸(IsNext)ì¸ì§€ íŒë³„í•˜ëŠ” Taskë¥¼ ìˆ˜í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.  
ì•ì„œ ì‚´í´ë³¸ BERT êµ¬ì¡°ë¥¼ ë³´ë©´, ë§¨ ì•ì˜ ì¶œë ¥ í† í°ì´ Cì…ë‹ˆë‹¤. í•´ë‹¹ C í† í°ìœ¼ë¡œ ë¬¸ì¥ ìŒì´ ì°¸ì¸ì§€ íŒë³„í•˜ëŠ” NSP Taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.  
![bert](./bert-paper/0.png "C í† í°ìœ¼ë¡œ NSPë¥¼ ìˆ˜í–‰í•œë‹¤.")

### Pre-training Data

ì—°êµ¬íŒ€ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ BookCorpus(800M Words)ì™€ English Wikipedia(2500M Words)ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.  
ì´ë•Œ Wikipedia ë°ì´í„°ì…‹ì˜ ê²½ìš° í‘œë‚˜ ë„í‘œ ë“±ì€ ì œì™¸í•œ text passagesë§Œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë˜í•œ ê¸´ ë¬¸ë§¥ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•´ Billion Word Benchmarkì²˜ëŸ¼ ì„ì¸ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ corpusë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

## Fine-tuning

ì´ë²ˆì—ëŠ” Bert ëª¨ë¸ì„ Fine-Tuning í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.
Bert ëª¨ë¸ì˜ Fine-tuningì€ ë¹„êµì  ë‹¨ìˆœí•©ë‹ˆë‹¤. **Taskì— ì•Œë§ëŠ” ì…ë ¥ê³¼ ì¶œë ¥ ë°ì´í„°ë¡œ Bert ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ end-to-endë¡œ ì—…ë°ì´íŠ¸(íŒŒì¸íŠœë‹)í•˜ë©´ ë©ë‹ˆë‹¤.** ì´ë•Œ Token representationì€ token level task (sequence tagging, QA ë“±)ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ë˜í•œ [CLS] Representationì€ Classification (entailment, sentiment analysis ë“±)ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ BERTì˜ íŒŒì¸íŠœë‹ ê³¼ì •ì„ ë„ì‹í™”í•œ ê²ƒì…ë‹ˆë‹¤.
![bert-fine-tuning](./bert-paper/fine-tuning.png "BERT Fine-Tuning")

**íŒŒì¸íŠœë‹ì€ í”„ë¦¬íŠ¸ë ˆì¸ë³´ë‹¤ ì ì€ ë¹„ìš©ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì **ì´ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ í›„ìˆ í•  ì‹¤í—˜ì—ì„œë„ ì—°êµ¬íŒ€ì€ ëŒ€ë¶€ë¶„ì˜ downstream taskë¥¼ íŒŒì¸íŠœë‹í•˜ëŠ”ë° single Cloud TPUë¡œ 1ì‹œê°„ ì •ë„ ë°–ì— ì†Œìš”ë˜ì§€ ì•Šì•˜ë‹¤ê³  í•©ë‹ˆë‹¤.

## Experiments

ì—°êµ¬íŒ€ì€ 11ê°œì˜ NLP Taskì— ëŒ€í•˜ì—¬ BERT íŒŒì¸íŠœë‹ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

1. **GLUE(General Language Understanding Evaluation)**

- ëª¨ë¸ì˜ ìì—°ì–´ ì´í•´ ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ 9ê°€ì§€ Taskë¡œ êµ¬ì„±ëœ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.
- GLUEì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ [ê¸€](https://huffon.github.io/2019/11/16/glue/)ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.

2. **SQuAD (Stanford Question Answering Dataset)**

- Wikipediaë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§Œë“  ì§ˆì˜ì‘ë‹µ(QA) Task ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.
- 2.0ì€ 1.1ì„ ê°œì„ í•œ ë²„ì „ìœ¼ë¡œ ë¬¸ì œë¥¼ ë³´ë‹¤ í˜„ì‹¤ì ìœ¼ë¡œ ê°€ê³µí•˜ì˜€ìŠµë‹ˆë‹¤.

3. **SWAG(Situations With Adversarial Generation)**

- ì• ë¬¸ì¥ì´ ì£¼ì–´ì§€ê³ , ë³´ê¸°ë¡œ ì£¼ì–´ì§€ëŠ” 4ê°€ì§€ ë³´ê¸° ì¤‘ì— ê°€ì¥ ì ì ˆí•œ ë¬¸ì¥ì„ ê³ ë¥´ëŠ” Taskì…ë‹ˆë‹¤.

ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![bert-fine-tuning](./bert-paper/gleu.png "GLEU Task")
![bert-fine-tuning](./bert-paper/squad1.png "SQuAD 1.1 Task")
![bert-fine-tuning](./bert-paper/squad2.png "SQuAD 2.0 Task")
![bert-fine-tuning](./bert-paper/swag.png "SWAG Task")

**BERTëŠ” ëª¨ë“  Taskì—ì„œ SOTA ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.** ğŸ˜Š (êµ¬ê¸€ì˜ ê¸°ìˆ ë ¥ì€ ë„ëŒ€ì²´ ì–´ë””ê¹Œì§€...)

## Ablation Studies

BERTëŠ” MLMì´ë‚˜ NSP ë“± ë‹¤ì–‘í•œ ìš”ì†Œë¥¼ ëª¨ë¸ì— ì ìš©í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒì„ ì´ë£¨ì–´ëƒˆìŠµë‹ˆë‹¤.
ì—°êµ¬íŒ€ì€ BERTì—ì„œ **ì–´ë–¤ ìš”ì†Œê°€ ì„±ëŠ¥ í–¥ìƒì— ì–´ë–»ê²Œ ê¸°ì—¬í–ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œì•„ë³´ê¸° ìœ„í•´ Ablation studyë¥¼ ìˆ˜í–‰**í•©ë‹ˆë‹¤.

> Q. Ablation studyê°€ ë­ì£ ?  
> A. Ablation studyëŠ” ëª¨ë¸ì´ë‚˜ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬ì„±í•˜ëŠ” ë‹¤ì–‘í•œ êµ¬ì„±ìš”ì†Œ(component) ì¤‘ ì–´ë– í•œ â€œfeatureâ€ë¥¼ ì œê±°í•  ë•Œ, ì„±ëŠ¥(performance)ì— ì–´ë– í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ íŒŒì•…í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤. (ì¶œì²˜: https://sangminwoo.github.io/glossary/atod)

### Effect of Pre-training Tasks

ì—°êµ¬íŒ€ì€ ì•„ë˜ì™€ ê°™ì€ ëª¨ë¸ë¡œ ê¸°ì¡´ ëª¨ë¸ê³¼ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

1. **NO NSP**  
   ê¸°ì¡´ ëª¨ë¸ì—ì„œ NSPë¥¼ ì œê±°í•©ë‹ˆë‹¤.
2. **LTR(Left To Right) & NO NSP**  
   ê¸°ì¡´ ëª¨ë¸ì—ì„œ MLMê³¼ NSPë¥¼ ì œê±°í•©ë‹ˆë‹¤.
3. **LTR(Left To Right) & NO NSP + BiLSTM**  
   ê¸°ì¡´ ëª¨ë¸ì—ì„œ MLMê³¼ NSPë¥¼ ì œê±°í•˜ê³ , ëª¨ë¸ ìœ„ì— ì„ì˜ë¡œ ì´ˆê¸°í™”ëœ BiLSTMì„ ì¶”ê°€í•©ë‹ˆë‹¤.

ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
![bert-fine-tuning](./bert-paper/ab1.png "NO NSP & LTR ì„±ëŠ¥ ë¹„êµ")

1. **NO NSP**
   QNLI, MNLI ê·¸ë¦¬ê³  SQuAD 1.1 ì„±ëŠ¥ì´ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤.
2. **LTR(Left To Right) & NO NSP**  
   ëª¨ë“  Taskì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ ë°œìƒí•˜ì˜€ìœ¼ë©°, íŠ¹íˆ MRPCì™€ SquADì—ì„œ í° í­ìœ¼ë¡œ ì„±ëŠ¥ì´ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤.
3. **LTR(Left To Right) & NO NSP + BiLSTM**  
   BiLSTMì„ ì¶”ê°€í–ˆì„ ë•Œ SQuADì˜ ì„±ëŠ¥ì´ í–¥ìƒí•˜ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” **ì–‘ë°©í–¥ í•™ìŠµì´ QAì™€ ê°™ì€ ë‘ ë¬¸ì¥ê°„ì˜ ê´€ê³„ê°€ ì¤‘ìš”ì‹œë˜ëŠ” Taskì— í° ì˜í–¥**ì„ ë¯¸ì¹˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì—°êµ¬íŒ€ì€ **ELMOì²˜ëŸ¼ LTRê³¼ RTL ëª¨ë¸ ê°ê° í•™ìŠµí•˜ì—¬ representationì„ í•©ì¹˜ëŠ” ë°©ì‹**ë„ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ **ì„¸ ê°€ì§€ ë¬¸ì œì **ì´ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

- single bidirectional modelë³´ë‹¤ ë‘ ë°°ì˜ ê³„ì‚°ëŸ‰ì´ í•„ìš”í•˜ë‹¤.
- RTLì˜ ê²½ìš° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ QAì™€ ê°™ì€ Taskì˜ ê²½ìš°ì— ì§ê´€ì ì´ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ëª¨ë“  ë ˆì´ì–´ì—ì„œ ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ ì»¨í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— deep bidirectional modelì— ë¹„í•´ ì„±ëŠ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. (\*)

> (\*)ì‚¬ì‹¤ ì„¸ë²ˆì§¸ ë¬¸ì œì ì€ ì´í•´ê°€ ì˜ ë˜ì§€ ì•Šì•„ì„œ ë…¼ë¬¸ ì›ë¬¸ë„ ê°™ì´ ì²¨ë¶€í•©ë‹ˆë‹¤!  
> this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.

### Effect of Model Size

ì—°êµ¬íŒ€ì€ ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ íŒŒì¸íŠœë‹ ì„±ëŠ¥ ì°¨ì´ë„ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤.  
ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![bert-fine-tuning](./bert-paper/ab2.png "Model Sizeë³„ ì„±ëŠ¥ ë¹„êµ")

**ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆê°€ í¬ë©´ í´ìˆ˜ë¡ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.** ë˜í•œ ì‚¬ì´ì¦ˆê°€ í° ëª¨ë¸ì€ small scale taskì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

### Feature-based Approach with BERT

ë§ˆì§€ë§‰ìœ¼ë¡œ BERTë¥¼ Feature-basedë¡œ ì‚¬ìš©í–ˆì„ ë•Œ ì„±ëŠ¥ì„ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤.

![bert-fine-tuning](./bert-paper/ab3.png "Feature-based BERT ì„±ëŠ¥ ë¹„êµ")

BERTë¥¼ **Feature-basedë¡œ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì´ì **ì´ ìˆìŠµë‹ˆë‹¤.

- Transformer Encoder ìƒì— ëª¨ë“  NLP Taskë¥¼ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ Feature-based ë°©ì‹ìœ¼ë¡œ task-specific modelì„ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì—°ì‚°ì´ ë¬´ê±°ìš´ í•™ìŠµ ë°ì´í„°ë¥¼ í•œë²ˆ pre-computeí•˜ì—¬ representationì„ ìƒì„±í•˜ê³ , ì´ ìœ„ì— ì—°ì‚°ì´ ê°€ë²¼ìš´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë§ì€ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ëŠ” ì‹ìœ¼ë¡œ Computational Benefitì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë˜í•œ ìœ„ í‘œì—ì„œ Concat Last Four Hiddenì˜ ê²½ìš° SOTA ëª¨ë¸ì¸ Fine-tuning BERT Baseì™€ 0.3 F1ë°–ì— ì°¨ì´ë‚˜ì§€ ì•ŠëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
**ì´ëŠ” BERTê°€ Fine-tuningë¿ë§Œ ì•„ë‹ˆë¼ Feature Basedì—ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.**

## Conclusion

- **Deep Bidirectional Model**ì„ í†µí•´ ê°™ì€ pre-train ëª¨ë¸ë¡œ **ëª¨ë“  NLP Taskì—ì„œ SOTAë¥¼ ë‹¬ì„±**í•  ìˆ˜ ìˆì—ˆë‹¤.
- pre-train ëª¨ë¸ì„ í†µí•´ ì ì€ resourceë¡œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤.

## Reference

- í•œêµ­ì–´ ì„ë² ë”© (ì´ê¸°ì°½)
- [https://misconstructed.tistory.com/43](https://misconstructed.tistory.com/43)
- [https://chloelab.tistory.com/25](https://chloelab.tistory.com/25)
- [https://sangminwoo.github.io/glossary/atod](https://sangminwoo.github.io/glossary/atod)
